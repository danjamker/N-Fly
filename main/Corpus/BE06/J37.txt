1. Introduction
The systematic review has underpinned evidence-based medicine, in which health care professionals are encouraged to use up-to-date and relevant scientific evidence to aid the clinical decision making process. Systematic reviews are often viewed as a “gold standard” of research [1] , and have instant appeal to clinicians who may not have the time, access, or skills to interpret individual studies. The use of a comprehensive, reproducible search strategy can reassure clinicians that all relevant literature is likely to have been identified. However, it is the extension of this systematic methodology to the critical appraisal and synthesis of original studies that arguably represents the greatest advantage over traditional narrative reviews. The judgement of external and internal validity of original studies, and the selection and weighting of results based on these features, is critical if the conclusions and recommendations of systematic reviews are to be believed by clinicians. However, this assessment of internal and external validity may be made more difficult because of the absence of complete and accurate reporting of original studies. 
Quality assessment is well established in systematic reviews of randomized controlled trials. A large number of different assessment tools exist, with a review by Moher et al. [2] identifying 25 scales and nine checklists developed to assess the quality of randomized controlled trials. A more recent estimate suggests that between 50 and 60 quality scales exist [3]. Despite the large numbers of tools available, there is no universally accepted gold standard. Some attempts have been made to improve the validity of the quality assessment scales, such as the use of a Delphi technique [4] and [5] to aid consensus, but the reliability of many lists is currently unknown [3]. QUADAS (Quality Assessment of Diagnostic Accuracy Studies) has recently been developed, using a Delphi technique and literature search, to assist in the quality assessment of studies of diagnostic accuracy [6]. 
The use of quality assessment tools to appraise observational studies included in systematic reviews is less well established than in systematic reviews of RCTs [7]. As no accepted gold standard exists, researchers either ignore the issue or develop their own tools [7]. Using quality assessment tools has several advantages. It allows original research to be systematically appraised and evaluated, and can be used to rank, weight, or score studies. Components are often generic and allow even novice researchers to appraise original material. However, each study is unique and a quality checklist may not include items that are relevant to consider for that particular study, or include irrelevant or useless items. This lack of flexibility could result in a misclassification of study quality. Creating additional criteria that are study specific may overcome this problem, although inherent problems exist with this. Even when a scale is used, subjective assessment is still required, as most checklists use terms such as “adequate” and “appropriate” to describe criteria [7]. 
Methodologic quality refers to the extent to which all aspects of a studies design and conduct can be shown to protect against systematic bias, nonsystematic bias, and inferential error [8]. It is therefore not surprising that checklists used to appraise observational studies tend to concentrate on issues of external and internal validity, although no empirical evidence exists to support this. Core domains include the comparability of subjects, details of intervention and exposures, outcome measurement, statistical analysis, and funding or sponsorship [9]. 
Considering observational studies is important, as clinicians will often have important questions that cannot be answered by randomized controlled trials or diagnostic studies. These include issues of prognosis, etiology, and risk, which are often best addressed by utilizing an observational design. Whereas the quality assessment of original studies included in systematic reviews of randomized controlled trials is widely accepted as constituting good practice, less is known about the status of quality assessment of observational studies. Although a variety of quality checklists for such studies have been developed [[10] and [11]; see also www.york.ac.uk/inst/crd/pdf/crd4_ph5.pdf], it is unclear if these tools are currently being used in the assessment of quality in systematic reviews. Given this, how is quality currently being assessed? 
We adopted the perspective of a busy clinician seeking to identify a recent systematic review of observational studies. Our aim was to describe what level of quality assessment the clinician might encounter. 
2. Methods
A PubMed search was performed on July 14, 2005, using the key words “systematic review” and “observational studies.” The abstracts of all papers published in the years 1999–2000 and 2003–2004 were screened for eligibility. Articles meeting the full inclusion criteria were then analyzed for the presence and nature of any quality assessment. Any article with a description of a quality assessment process was included. This search was not intended to be systematic, but rather was designed to reproduce the type of material that may be identified by a clinician's search. 
2.1. Inclusion criteria
To be included in this study the article had to meet the following criteria: 
• Published in a peer-reviewed journal
• English language
• Described by the authors as a systematic review
• Include observational studies
• Human participants
2.2. Data extraction
All articles retrieved were examined by one reviewer (C.M.) for the presence and nature of quality assessment. 
3. Results
A total of 32 systematic reviews were identified for the period 1999–2000, of which 27 met the inclusion criteria. For the period 2003–2004, 98 reviews were identified, of which 78 met the inclusion criteria. 
Six (22%) of the 1999–2000 articles described quality assessment of original studies, compared with 39 (50%) of articles published in 2003–2004 (see Table 1). 
Table 1. 
Quality assessment 1999–2000 
In 1999–2000, all reviews that assessed quality devised their own quality assessment criteria. In 2003–2004, 10 different tools were used (Table 2). Two studies did not report their method of quality assessment. More general medical journals included reviews with quality assessment than specialist medical journals (Table 3). The criteria used to assess quality were extracted and the common items presented in the Table 4. 
Table 2. 
Type of quality assessment tools used 
a Total of 40, as one article used two methods to assess quality. 
Table 3. 
Quality assessment and journal type 
Table 4. 
Criteria used to assess quality (2003–2004) 
4. Discussion
This study has examined the current practice of quality assessment in systematic reviews of original observational studies. There were three main findings: the increase in quality assessment activity from 1999–2000 to 2003–2004, the absence of quality assessment in almost half of the most recently published studies, and a lack of consensus in the choice of quality assessment tool in the remainder. 
Of the 78 most recently published reviews that were analyzed, 39 (50%) did not report using any formal method of quality assessment. Does this actually matter? If a systematic review is to offer an advantage over the narrative review, then the original research on which it is based needs to be explicitly, thoroughly, and systematically appraised for internal and external validity. The absence of quality assessment implies that studies are being given equal weight regardless of quality, and this may lead to misleading conclusions. 
The inclusion of poor quality studies may provide misleading conclusions that cannot be used to accurately answer a clinician's query [12]. For systematic reviews to be a useful adjunct to making clinical decisions, it is essential that clinicians can have confidence in the findings reported. The absence of quality assessment implies that studies are being included in the review and given equal weight in the analysis regardless of quality, and this seems to offer little advantage over the traditional narrative review. 
Another finding in this study was the lack of consensus about the method of assessing quality between those reviews that actually carried it out. Of the 39 reviews that did assess quality, 10 different forms of assessment were used, ranging from tools specifically designed to assess quality in that review, instruments published in peer-reviewed journals, and simply grading the study on the basis of the study design. Does it matter that different quality assessment tools are being used? If they are of sufficiently high quality, then this is less of a concern. However, no study has done a head-to-head comparison of these different instruments, and it is possible that different assessment tools, particularly those using a scoring system, will produce different assessments of quality, as has been demonstrated with randomized controlled trials [12]. Having a variety of quality assessment tools in use makes it more difficult for a clinician to judge or be confident about the conclusions of reviews. Having no universally accepted quality assessment tool raises other problems. Not only is it difficult to compare the results of similar reviews, but also tools such as the MOOSE guidelines are being used to assess quality, when they were actually developed as reporting guidelines. 
Although a variety of different tools were used to assess quality, Table 4 demonstrates that a broad consensus as to what should be included in a quality assessment is emerging. Two different themes are evident: issues of bias assessment and complete reporting. Criteria highlighted as being commonly considered important include the use of accurate and appropriate outcome measures, adjustment for confounding, the appropriate selection of controls, assessment of loss to follow-up, and appropriate statistical analysis. Should a consensus assessment tool be developed these may all be considered potential candidates for inclusion, as they are already commonly being used. 
This underlines the two aspects of quality review that are likely to be important: has a study reported completely and accurately what has been done, and was any quality assessment appropriate for the study question. The assessment of quality overall is made difficult if reporting is inadequate. This has recently been highlighted by Pocock et al. [13]. Several recent initiatives, including the CONSORT statement [14] (the Consolidated Standards for Reporting of Trials), the QUOROM statement [15] (the Quality of Reporting of Meta-analysis), the TREND statement [16] (Transparent Reporting of Evaluations with Nonrandomized Designs), and the STARD initiative [17] (the Standards for Reporting of Diagnostic Accuracy) have aimed to improve the quality of reporting of original research, and although the STROBE group (www.STROBE-statement.org) (Standards for the Reporting of Observational studies) have recently met, no guidance is yet available (www.STROBE-statement.org). Evidence exists that the CONSORT statement has improved the reporting of the randomized controlled trial [18], although previous attempts to improve the reporting of observational studies [19], [20], [21] and [22] did not appear to have a major impact. 
Differences have been observed between quality assessment and the type of journal publishing the review (Table 3). Cochrane and HTA reviews were excluded from this study, as they already have a formal quality assessment procedure. Quality assessment (2003–2004) is more common in general medical (69%) and epidemiology (66%) journals than in specialist medical publications (44%), although this needs to be interpreted with care due to small number of epidemiology reviews included. There is also a difference demonstrated in the mean impact factors of the 2003–2004 journals, with a higher mean impact factors in journals with quality assessment (4.23) than those without (3.34). This trend is more evident in the 1999–2000 sample with the mean impact factor of 6.17 in journals assessing quality, compared with 3.12 in those not. This may be coincidental, given the relatively small number of reviews included in this study. There is currently no obvious editorial policy on quality assessment that the authors could identify. Specialist medical journals have seen the largest increase in the amount of systematic reviews including quality assessment, increasing from 17% in 1999–2000 to 44% in 2003–2004. 
This study has demonstrated that quality assessment of original material included in systematic reviews is not routinely occurring, and when it does occur a variety of different methods are used. If clinicians are to rely on the results of systematic reviews, they need to be confident that the results reflect both a thorough search and a systematic and credible basis for weighting results from different studies. This is difficult in the absence of universally accepted tools of quality assessment, and hampered further by a lack of complete and accurate reporting. The STROBE statement represents a step towards improving the reporting of observational studies. A statement on quality assessment of observational studies is needed as well. 

