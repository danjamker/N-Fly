
Evaluation of fusion techniques is a non-trivial task especially as it is often difficult to say which of two fused images are better without reference specific tasks. For example, the best image for a task involving tracking should emphasise the objects to be tracked. However, this will probably not be the best approach if perceptually natural images are required. Evaluation techniques are generally split into subjective or qualitative tests and objective or quantitative tests. The majority of testing of image fusion techniques tends to be subjective based on psycho-visual testing. The main disadvantage of this technique is that it requires time and resources (often in the form of expert subjects). Quantitative testing overcomes these difficulties, in theory, however it is difficult to perform and metrics must be validated. A number of assessment methods, such as root mean squared error, require a “ground truth” fused image which does not usually exist for real images (e.g. [6]). 
Three metrics are considered in this paper, which do not require ground truth images: mutual information (MI): described in [27], this essentially computes how much information from each of the fused images is transferred to the fused image. The Xydeas and Petrovic QAB/F metric, proposed in [28], considers the amount of edge information transferred from the input images to the fused images using a Sobel edge detector to calculate the strength and orientation information at each pixel in both source and the fused images. The final metric, presented by Piella and Heijnmans in [29], is based on an image quality metric (IQM) introduced in [30]. This uses three measures: correlation, luminance distortion and contrast distortion. To apply this to fusion, saliency information in arbitrary windows is used to weight a comparison of all input images to the fused image. In order to include edge information in the calculation, this information is calculated from both the original images and from “edge images”. 
5. Results and discussion
Fig. 9 shows the fused result of the IR and visible images from the “UN Camp” sequence for the region-based method. As can be observed, the region level scheme performs comparably with the pixel-based fusion method, shown in Fig. 3. The input images have been jointly segmented and priority based on either entropy, variance or activity. Each of these methods have successfully chosen the majority of the background of the fused image from the visible image, while the important features in the IR image, namely the figure, have also been included. With these images, there appears to be very little differentiating the three methods of calculating priority. The low-pass coefficients are combined with both averaging and using a region-based combination based on the mask used to fuse the high-pass coefficients. The region-based image method produces images with better contrast that more accurately reflects the contrast of the original images compared with both pixel-based fusion and region-based fusion with averaging of the low-pass bands. 
Fig. 9. Region-based fusion of the “UN Camp” sequence. (a) Using entropy to calculate priority with low-pass averaged, (b) using entropy to calculate priority with low-pass region-fused, (c) using variance to calculate priority with low-pass region-fused, (d) using activity to calculate priority with low-pass region-fused, (e) mask used as multiplier on IR coefficients and (f) region-fused image with masked IR coefficients multiplied by a factor of 2.0. 
Fig. 9(e) shows the weighting mask generated by extracting all regions in the IR image above a certain threshold. This can be seen to successfully extract the feature representing the figure. The coefficients representing the figure are weighted by a factor 2.0, which accentuates the presence of the figure, as shown in Fig. 9(f). 
Fig. 10 shows two multi-focus “Pepsi” images. The images are jointly segmented and the priorities calculated using entropy are shown in the figure. The parts of the image that are in focus have higher priorities than the out of focus parts. Thus, the fused image shows the entire image in focus. There is no perceivable difference between the pixel- and region-based fusion algorithms. As the two images are from the same type of sensor, there is no advantage to performing the fusion of the low-pass coefficients in a region-based manner compared with averaging. 
Fig. 10. Fusion of multi-focus Pepsi images. (a) Input image A, (b) input image B, (c) PB:AVE fused image, (d) PB:PYR fused image, (e) PB:DWT fused image, (f) PB:CWT fused image, (g) RB:CWT fused image, (h) entropy priority map for image A for the jointly segmented input images and (i) entropy priority map for image B for the jointly segmented input images. 
Fig. 11 shows three input images, from the “Sea” data set, depicting a ship at sea. The data set consists of two IR images covering different spectral bands and a visible image. The pixel-fused image has pulled through some of the interlace artefacts and noise from the images to a greater extent than the region-based method. Errors due to mis-registration of the input images, which are not as visible in the region-fused image. However, some detail around the boat is missing in the region-fused image. While the region-fusion of the low-pass coefficients has improved contrast averaging, the image is more difficult to interpret. This is partly due to the fact that the sea in front of the boat on the right is from a different image to that on the left. 
Fig. 11. Fusion of the “Sea” images. (a) 2–3 ?m frequency band IR input image, (b) 8–12 ?m frequency band IR input image, (c) visible range CCD image, (d) PB:AVE fused image, (e) PB:PYR fused image, (f) PB:DWT fused image, (g) PB:CWT fused image, (h) RB:CWT fused image averaging low-pass coefficients and (i) RB:CWT image with region-based low-pass fusion. 
Table 1 shows a comparison of this region-based (RB:CWT) fusion method and a number of pixel-based fusion methods using the metrics described in Section 4. These results confirm that the region fusion algorithm performs comparably to pixel-based algorithms. MI consistently rates region-based fusion as producing better results than pixel-based, except for the “Sea” images when it is rated second and averaging is rated highest. However, perceptually, averaging produces the worst fused image. The Xydeas and Petrovic metric shows the RB:CWT as out performing pixel-based methods, while the Piella metric shows the PB:CWT method to produce slightly better results. We have found that QAB/F and IQM generally correlate well with the results of visual analysis, however, it should be noted that these metrics are based on edges and fused images containing significant artefacts such as ringing introduced by the transform can be rated highly by these metrics but look inferior perceptually. Ideally the fused images should be tested using the performance of human operators at performing specific tasks. We hope to carry out such tests in the future. 
Table 1. 
Comparison of fusion methods 
Fig. 12 shows the fusion of the “Octec” IR and visible images. The input IR and visible images, with their segmentation overlaid to show the regions, are given in Fig. 12(b) and (a) respectively. In the pixel-fused image, Fig. 12(d), the figure, visible in the IR image is obscured by the smoke. As the smoke is fairly textured, the pixel-based fusion and region-based fusion using a statistically based priority pull through the smoke from the visible image obscuring the figure. Fig. 12(c) shows the RB:CWT fused image with the low-pass coefficients also region fused. This image has much better contrast compared with the PB:CWT fused image and perceptually looks better. However, there are some artefacts, for example in the sky in the top left of the image. As regions of interest tend to be small while the background (and other less interesting features in an image) large, the inverse of the size of a region can be used for the priority. In the region-based fused image, Fig. 12(e), using the inverse of size as a priority, the figure is clearly visible. 
Fig. 12. Octec IR and visible fused images. (a) Octec IR image with segmentation overlaid, (b) Octec visible image with segmentation overlaid, (c) region-fused image using entropy as priority, (d) PB: CWT fused image, (e) region-fused image using size as priority. 
Fig. 13 shows some fused images from the “UN Camp” sequence. The distance between the centre of masses (see Fig. 8) of the region representing the road and the figure is calculated and the coefficients of the figure are weighted inversely proportional to the closeness to the road. For this experiment the road was manually selected and the figure detected by thresholding the IR image. The calculated distance (given in pixels) and the weighting applied to the figure is given for each image. These images are jointly segmented and fused using an entropy priority. The figure is seen to get brighter as he moves closer to the road. This task differs from that originally proposed in [12] for this image sequence, which was to determine the figure's position relative to the fence. However, this is a very difficult task and the task proposed is a worthwhile exercise showing how this algorithm could be applied. 
Fig. 13. The figure in the IR image from the “UN Camp” sequence is highlighted depending on closeness to the road. (a) Image no. 11, Dist = 220; weighting = 1.10. (b) Image no. 13, Dist = 213; weighting = 1.13. (c) Image no. 18, Dist = 207; weighting = 1.16. (d) Image no. 19, Dist = 111; weighting = 1.65, (e) Image no. 24, Dist = 105; weighting = 1.67 and (f) image no. 26, Dist = 101; weighting = 1.69. 
While the region fusion approach would not be applicable in all situations the aim is to show how region-based rules can be easily applied in a number of situations to improve the resulting fused image. The more prior knowledge available about the scene, the more we can tailor these rules to make them more useful. For example, the three “Sea” images in Fig. 14 show three kayaks in various formations. The distance between each kayak can be calculated and their spatial arrangement determined and using some prior knowledge decide what kind of boats are being dealt with (e.g. terrorists, smugglers, holiday makers etc.). 
Fig. 14. Overlay of regions on three images from the “Sea” sequence. 
All results in this paper were obtained with a Matlab implementation of the algorithm as part of the Image Fusion Toolbox (IFT) being developed at the University of Bristol and generated using a computer with a Pentium (R) 4 2.66 GHz processor and 1 GB of RAM. The RB:CWT fusion for the “UN Camp” images were computed in 37.90 s (0.64 s for the DT-CWT transform; 30.5 s for the segmentation; 6.38 s for the fusion; and 0.38 s for the inverse DT-CWT). To date, no effort has been made to optimise the algorithm and it is expected that substantial improvements in efficiency could be made. The PB:CWT takes 1.60 s (0.64 s for the DT-CWT transform; 0.58 s for fusion; and 0.38 s for the inverse DT-CWT). The PB:DWT takes 0.57 s (0.31 s for the DWT; 0.15 s for fusion; and 0.12 s for the inverse DWT); the PB:PYR requires 0.43 s (0.21 s for the forward transform; 0.18 s for fusion; and 0.04 s for the inverse transform) and PB:AVE takes 0.01 s. While the RB:CWT takes significantly longer to produce results, most of this time is taken by segmentation. This is often a post-processing step for pixel-based fusion and this must be considered when comparing timings. 
6. Conclusions
This paper has demonstrated that comparable results can be achieved with region and pixel-based methods. While region-based fusion methods are generally more complex, there are a number of advantages of such schemes over pixel-based fusion. The advantages demonstrated here include novel more intelligent fusion rules and the ability to treat regions differently depending on a variety of properties such as average activity, size and position relative to other regions in the scene. Future work should include a thorough objective and subjective testing of the effects of using either joint or separate multi-modal image segmentation and the further development of higher-level region-based fusion rules. 

